# Project: Data Pipeline with Airlfow
## Data Pipeline to manage ETL process from Amazon S3 to Amazon Redshift with Apache Airflow 

## Context of the project
Sparkify is startup that runs a new music streaming app. Their data analytics team wants to do analyses on song listening behavior to further the business. Analyses could be done to improve user experience and sales. Until now access to data was difficult due to storage in JSON file. Goal is to provide data in way that enables data analysts. Sparkify wants to load its data from S3 to Redshift using Apache Airflow for the ETL pipeline

## Tables
**Table songplays:**
Records in log data associated with song plays i.e. records with page NextSong
Fact table

**Table users:**
Users in the app
Dimension table with user_id ad PRIMARY KEY which identifies a user
Information about users can change. E.g. a user could switch from level 'free' to 'paid' or change the last name

**Table songs:**
Songs in music database
Dimension table
Information on songs do not change over time

**Table time:**
Timestamps of records in songplays broken down into specific units
Dimension table

**Table artists:**
Artists in music database
Dimension table
Information about artists can change. E.g. name or location can change

## ETL pipeline
1. Load data from S3 to Staging tables in Redshift using `StageToRedshiftOperator`
2. Load data from staging tables to Fact table `songplays` using `LoadFactOperator`
3. Load data from staging tables to Dimension tables using `LoadDimensionOperator`
4. Perform data quality checks in Redshift comparing result from provided SQL queries to expected result
![Visualization of Airflow DAG](/DAG.png)

## Files in S3
**Song dataset (as per project specifications)**
Song data is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID

**Log dataset (as per project specifications)**
Log files are in JSON format generated by event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.
The log files in the dataset you'll be working with are partitioned by year and month. Log files populate tables time, users and songplays

## Additional files
`create_tables.sql`: Create table statements for Redshift

## Run the project from locally-installed Apache Airflow
1. Create cluster (dc2.large with 2 nodes is sufficient) on Redshift allowing public access and all TCP connections on port 5439; database name: dev
2. Create tables using sql statements in `plugins/helpers/create_tables.sql`
3. Install Apache Airflow with S3 module
    `pip install apache-airflow[s3]`
4. Run the server and scheduler
    `./airflow-start.sh`
5. Create connections in Airlfow
    * Open http://localhost:8080 in your browser and go to 'Admin' - 'Connections'. Create the following connections
        * 5.1. AWS credentials:
            * Conn Id: `aws_credentials`
            * Connection type: `Amazon Web Services`
            * Login: Your AWS ACCESS KEY
            * Password: Your AWS SECRET ACCESS KEY
        * 5.2. Redshift connection:
            * Conn Id: `redshift`
            * Conn Type: `Postgres`
            * Host: Endpoint of your Redshift cluster created in step 1 without the port and ':' at the end
            * Schema: `dev`
            * Login: `awsuser`
            * Password: Password you set when launching your Redshift cluster
            * Port: `5439`
6. Set DAG to 'On'
    In http://localhost:8080/admin/ click on the "Off" box left of dag to activate it
7. Refresh the page to see the progress of the DAG
8. Turn DAG off and delete your cluster to avoid unnecessary costs
